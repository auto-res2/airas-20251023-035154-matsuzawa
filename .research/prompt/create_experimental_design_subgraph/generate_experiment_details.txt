
LLM Name: o3-2025-04-16
Input:
You are an AI researcher. You will conduct experiments to demonstrate the superiority of the new method described in # New Methods. Please output all information required to implement the experiments according to the format specified in # Output Format. The section # Experimental Environment describes the computational environment available for this experiment.

# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "In BOIL, the acquisition score is obtained by dividing Expected-Improvement (EI) by an estimated cost coming from a simple linear regression.  However, EI is computed only from the GP predictive mean; it completely ignores the predictive variance of the final performance that the GP supplies.  As a consequence BOIL often proposes configurations whose *expected* gain is large but whose *uncertainty* is also large, leading to wasted wall-clock time on unstable or highly noisy runs.  A very small change that explicitly trades off improvement against predictive risk can reduce these wasted evaluations without touching the GP model, the curve-compression, or the selective data-augmentation machinery.",
    "Methods": "Stability-Aware Cost-Efficient BOIL (S-BOIL)\nMinimal change: multiply the utility part of BOIL’s cost-aware acquisition by an exponential penalty that depends on the GP’s predictive variance σ²(x,t) at the candidate (x,t):\n\n    Utility_s = EI(x,t) * exp(-β · σ²(x,t))\n    Acquisition  =  log(Utility_s)  –  log(EstimatedCost+0.1)\n\nwhere β≥0 is a single tunable constant (β≈1 works well).  Intuition: for equal EI, prefer points the GP is certain about; for equal variance, recover the original BOIL rule.  Because σ² is already returned by the GP, the only code change is three lines in the acquisition function.  No retraining or extra hyper-parameters are introduced apart from β.",
    "Experimental Setup": "Base method: original BOIL implementation.\nProposed: S-BOIL (β ∈ {0.5,1}).\nTask: tune two hyper-parameters (learning-rate, hidden-units) and the training-iteration budget for an MLP on the Fashion-MNIST 10-class task (using PyTorch).  Each BO run is allowed 30 real evaluations; wall-clock time is logged.\nMetrics:\n1. Best validation accuracy reached vs. number of real evaluations.\n2. Total wall-clock time until a target accuracy (80%) is hit.\nProtocol: 10 independent seeds for each method.  Same initial random design (3 points).",
    "Experimental Code": "# --- patch to BOIL: Stability penalty ------------------------------------\n# in utility_cost_evaluation_single() *replace* the EI computation line\n# original: utility = acq_func.acq_kind(x,gp=self.gp)\n# new lines:\nmean, var = self.gp.predict(x.reshape(1,-1), return_var=True)\nei_val  = acq_func.ei(mean, var, acq_func.best_y)   # unchanged helper\nutility = ei_val * np.exp(-beta * var)              # <- stability term\n# everything else stays the same\n# ----------------------------------------------------------\n\n# Minimal runnable demo (requires original BOIL code in path)\nimport torch, torchvision, torch.nn as nn, torch.optim as optim\nfrom boil import BOIL        # assume original class name\n\nbeta = 1.0  # stability weight\nbo = BOIL(func=mlp_runner, SearchSpace={\"lr\":(1e-4,1e-1),\n                                        \"units\":(64,512),\n                                        \"T\":(1,15)})\nbo.beta = beta               # pass to acquisition helper\nbo.init(n_init_points=3)\nfor _ in range(27):          # 30 evaluations total\n    bo.suggest_nextpoint()\nprint(\"Best acc =\", bo.Y_original.max())",
    "Expected Result": "Across 10 seeds S-BOIL is expected to reach 80% validation accuracy in ~15% less wall-clock time and 2 fewer function evaluations on average compared with BOIL.  The area-under-curve of best-seen accuracy vs. evaluations should also improve by ≈5–7%.",
    "Expected Conclusion": "A tiny, one-line modification that penalizes predictive variance lets BOIL avoid expensive yet uncertain configurations, giving faster convergence at virtually no additional computational cost or implementation complexity.  This demonstrates that explicitly accounting for GP uncertainty inside the acquisition utility—beyond its usual role in EI—is a simple but powerful way to improve efficiency in iterative hyperparameter optimization."
}

# MODEL LIST
{
    "Large Language Models": {
        "Llama-4-Scout-17B-16E": {
            "model_parameters": {
                "total_parameters": "109b",
                "active_parameters": "17b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "Llama-4-Maverick-17B-128E": {
            "model_parameters": {
                "total_parameters": "400b",
                "active_parameters": "17b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "Qwen3-0.6B": {
            "model_parameters": "0.6b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-0.6B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-1.7B": {
            "model_parameters": "1.7b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-1.7B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-1.7B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-4B": {
            "model_parameters": "4b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-4B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-4B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-8B": {
            "model_parameters": "8b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-8B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-8B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-14B": {
            "model_parameters": "14b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-14B",
            "language_distribution": "",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-32B": {
            "model_parameters": "32.8b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-32B",
            "language_distribution": "",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "DeepSeek-v3": {
            "model_parameters": {
                "total_parameters": "671b",
                "active_parameters": "37b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report},\n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437},\n}"
        },
        "DeepSeek-V3.1": {
            "model_parameters": {
                "total_parameters": "671B",
                "active_parameters": "37B"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "Text"
            ],
            "output_modalities": [
                "Text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report},\n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437},\n}"
        },
        "DeepSeek-V3.2-Exp": {
            "model_parameters": {
                "total_parameters": "671B",
                "active_parameters": "37B"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "Text"
            ],
            "output_modalities": [
                "Text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv32,\n      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention},\n      author={DeepSeek-AI},\n      year={2025},\n}"
        },
        "gpt-oss-20b": {
            "model_parameters": {
                "total_parameters": "21b",
                "active_parameters": "3.6b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/openai/gpt-oss-20b",
            "context_length": "",
            "language_distribution": "multilingual",
            "input_modalities": "text",
            "output_modalities": "text",
            "dependent packages": [
                "accelerate",
                "transformers",
                "kernels"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"openai/gpt-oss-20b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\nmessages = [\n    {\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    return_dict=True,\n).to(model.device)\n\ngenerated = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card},\n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925},\n}"
        },
        "gemma-3-1b-it": {
            "model_parameters": "1b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-1b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        },
        "gemma-3-4b-it": {
            "model_parameters": "4b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-4b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        },
        "gemma-3-27b-it": {
            "model_parameters": "27b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-27b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-27b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        }
    },
    "Vision Language Models": {},
    "Vision Language Action Models": {},
    "Diffusion Models": {}
}

# DATASET LIST
{
    "Text Datasets": {
        "alpaca-cleaned": {
            "discription": "",
            "num_training_samples": "",
            "num_validation_samples": "",
            "huggingface_url": "https://huggingface.co/datasets/yahma/alpaca-cleaned",
            "language_distribution": "",
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "databricks-dolly-15k": "",
        "gsm8k": {
            "discription": "A dataset of elementary school math word problems requiring 2 to 8 steps to solve",
            "num_training_samples": 7473,
            "num_validation_samples": 1319,
            "huggingface_url": "https://huggingface.co/datasets/openai/gsm8k",
            "language_distribution": "English",
            "dependent packages": [],
            "code": "",
            "citation": "@article{cobbe2021gsm8k,\n  title={Training Verifiers to Solve Math Word Problems},\n  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\n  journal={arXiv preprint arXiv:2110.14168},\n  year={2021}\n}"
        },
        "MATH": {
            "discription": "The MATH dataset consists of approximately 12,500 mathematics problems ranging from middle school to early university level. Each problem includes a natural language question, a detailed step-by-step solution, and a final answer, and it is widely used to evaluate large language models (LLMs) in terms of their abilities in mathematical reasoning, logical inference, and step-by-step problem solving.",
            "num_training_samples": 12500,
            "num_validation_samples": 0,
            "huggingface_url": "https://huggingface.co/datasets/qwedsacf/competition_math",
            "language_distribution": "English",
            "dependent packages": [],
            "code": "",
            "example": "{'problem': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.',\n 'level': 'Level 1',\n 'type': 'Counting & Probability',\n 'solution': 'The spinner is guaranteed to land on exactly one of the three regions, so we know that the sum of the probabilities of it landing in each region will be 1. If we let the probability of it landing in region $C$ be $x$, we then have the equation $1 = \\frac{5}{12}+\\frac{1}{3}+x$, from which we have $x=\\boxed{\\frac{1}{4}}$.'}",
            "data_structure": "A data instance consists of a competition math problem and its step-by-step solution written in LaTeX and natural language. The step-by-step solution contains the final answer enclosed in LaTeX's \boxed tag.\n- problem: The competition math problem.\n- solution: The step-by-step solution.\n- level: The problem's difficulty level from 'Level 1' to 'Level 5', where a subject's easiest problems for humans are assigned to 'Level 1' and a subject's hardest problems are assigned to 'Level 5'.\n- type: The subject of the problem: Algebra, Counting & Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra and Precalculus.",
            "citation": "@article{hendrycksmath2021,\n    title={Measuring Mathematical Problem Solving With the MATH Dataset},\n    author={Dan Hendrycks\n    and Collin Burns\n    and Saurav Kadavath\n    and Akul Arora\n    and Steven Basart\n    and Eric Tang\n    and Dawn Song\n    and Jacob Steinhardt},\n    journal={arXiv preprint arXiv:2103.03874},\n    year={2021}\n}"
        }
    },
    "Image Datasets": {
        "ImageNet": "",
        "CIFAR-10": ""
    }
}

# Output Format
- experiment_summary：
  - Describe the overall implementation details of the experiment. Summarize the purpose, components, and workflow so that the entire structure of the experiment can be clearly understood.
- evaluation_metrics：
  - List all evaluation metrics used in this experiment, including only their names, in a list format. (e.g., Accuracy AUC ROC, F1 Score, RMSE, BLEU, ROUGE, etc.)
- models_to_use：
  - Select 1 deep learning or machine learning models to be used in the experiment and output them in a list format.
  - Each model name should clearly indicate its number of parameters.
  - Refer to the provided “# MODEL LIST” for guidance, although models not included in the list are also acceptable.
  - If the proposed method itself introduces a new model (e.g., a novel architecture), return an empty list and describe the details of the method in new_method.
- datasets_to_use：
  - Select 1 datasets to be used in the experiment and output them in a list format.
  - Refer to the provided “# DATASET LIST” for guidance, although datasets not included in the list are also acceptable.
  - If a new dataset is proposed as part of this study, return an empty list and describe its details in new_method.
- proposed_method：
  - Describe the proposed method and its implementation in detail.
  - Clearly state its objectives, theoretical background, components, and algorithmic procedures.
- comparative_methods：
  - Select 1 existing methods for comparison with the proposed method and output them in a list format.
  - For example, if the proposed method is a new optimization algorithm, comparative methods might include Adam or AdamW.
  - If the proposal is a new LLM architecture, comparative methods might include Llama 4 or Qwen.
- hyperparameters_to_search：
  - Output a list of objects, where each object contains "name" (hyperparameter name) and "range" (search range).
  - For example: [{"name": "learning_rate", "range": "0.001-0.01"}, {"name": "batch_size", "range": "16,32,64"}, {"name": "weight_decay", "range": "0.0001-0.001"}]
  - Search ranges can be expressed as ranges (e.g., "0.001-0.01") or discrete values (e.g., "16,32,64").
Output:
{
    "experiment_summary": "The experiment aims to verify that the Stability-Aware Cost-Efficient BOIL (S-BOIL) acquisition rule accelerates hyper-parameter tuning compared with the original BOIL.  We run black-box optimisation on a two-layer MLP (≈1.2 M parameters) that is trained on Fashion-MNIST.  The optimiser must choose three configuration variables—learning-rate, hidden-units, and training-iteration budget—over 30 real training runs.  For each of 10 random seeds we execute (1) vanilla BOIL and (2) S-BOIL with a variance-penalised utility EI·exp(−βσ²).  β is treated as a small discrete hyper-parameter {0.5,1}.  All other code, GP surrogate, cost model, and initial design remain identical.  Metrics recorded at every function evaluation are: best-seen validation accuracy, wall-clock time, and the area-under-curve (AUC) of accuracy versus evaluations.  The primary comparisons are (i) mean evaluations and time to reach 80 % accuracy and (ii) mean AUAC across seeds.  The whole study fits comfortably on one A100 GPU, but eight are available if parallel seeds are desired.",
    "evaluation_metrics": [
        "Validation Accuracy",
        "Wall-clock Time to Target",
        "Area Under Accuracy Curve"
    ],
    "models_to_use": [
        "Two-Layer-MLP-1.2M"
    ],
    "datasets_to_use": [
        "Fashion-MNIST"
    ],
    "proposed_method": "Stability-Aware Cost-Efficient BOIL (S-BOIL)\nObjective: Reduce wasted evaluations on hyper-parameter configurations that have high expected improvement (EI) but also high predictive uncertainty.\nTheory: Original BOIL ranks candidates by log(EI)–log(cost).  S-BOIL adds a variance penalty, multiplying EI by exp(−βσ²) where σ² is the GP predictive variance and β≥0 controls strength.  This keeps the desirable cost-aware property while explicitly favouring stable (low-variance) points.\nAlgorithmic steps per iteration:\n1. Fit Gaussian-process surrogate to observed (configuration→performance) data.\n2. For each candidate (x,t) produced by BOIL’s search heuristics compute mean μ and variance σ².\n3. Compute utility_s = EI(μ,σ²)·exp(−βσ²).\n4. Acquisition = log(utility_s) – log(EstimatedCost+0.1).\n5. Return the candidate with maximal Acquisition; evaluate the true objective; append the result; repeat.\nImplementation: Requires only ~3 new lines inside BOIL’s acquisition function.  β is exposed as a user argument.  No retraining, extra models, or heavy computation is introduced.",
    "comparative_methods": [
        "BOIL"
    ],
    "hyperparameters_to_search": [
        {
            "name": "learning_rate",
            "range": "1e-4-1e-1"
        },
        {
            "name": "hidden_units",
            "range": "64-512"
        },
        {
            "name": "training_iteration_budget",
            "range": "1-15"
        },
        {
            "name": "beta",
            "range": "0.5,1"
        }
    ]
}
