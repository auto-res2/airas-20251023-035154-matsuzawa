
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} wandb.mode=online

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} trial_mode=true
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `trial_mode=true` (optional): Lightweight execution for validation (epochs=1, batches limited to 1-2, disable Optuna n_trials=0)
  * **When trial_mode=true, code must automatically set wandb.mode=disabled internally (e.g., `if cfg.trial_mode: cfg.wandb.mode = "disabled"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `{results_dir}/config.yaml`
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
    - Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Trial mode handling**: When `cfg.trial_mode=true`, automatically set `cfg.wandb.mode="disabled"` before any WandB operations

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251023-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Trial Mode Behavior**: When `trial_mode=true`, code must automatically set `cfg.wandb.mode="disabled"` internally
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: Code is NOT ready for execution.

Major blocking issues:
1. src/evaluate.py is truncated ‑ the file stops in the middle of the argparse definition and contains unmatched quotes/brackets, so it will not even import, let alone run the two-stage evaluation specified in the requirements.
2. Hydra configuration groups are missing.  config/config.yaml does not declare a `run` group in its defaults (e.g. `- run: default`).  Therefore the required CLI override `run={run_id}` will raise a ‘run is not a valid config group’ error.
3. Because of (2) the subprocess invocation built in src/main.py (`python -m src.train run=...`) will also fail for every experiment.

Until at least these issues are fixed, the code cannot be executed successfully in the experiment pipeline.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "# config/config.yaml \u2013 global defaults read by Hydra\n\n# Order is important: later defaults *override* earlier ones when merged\n# via \"run={id}\" on the command-line.\n\ndefaults:\n  - _self_\n\n###############################################################################\n#                             run-level stub                                  #\n###############################################################################\nrun:\n  run_id: default-run\n  method: baseline\n\n###############################################################################\n#                              WandB                                          #\n###############################################################################\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online            # overridden to \"disabled\" inside trial_mode\n\n###############################################################################\n#                           model defaults                                    #\n###############################################################################\nmodel:\n  name: two-layer-mlp-1.2m\n  input_dim: 784\n  hidden_units: 256\n  output_classes: 10\n  activation: relu\n  weight_init: kaiming_uniform\n\ndataset:\n  name: fashion-mnist\n  transforms:\n    - ToTensor\n    - Normalize(mean=0.5, std=0.5)\n  validation_split: 0.1\n\ntraining:\n  batch_size: 128\n  optimizer: adam\n  device: cuda\n  learning_rate: 1e-3\n  epochs_per_iteration: 5\n  num_workers: 4\n  pin_memory: true\n\nexperiment:\n  n_init_points: 3\n  total_evaluations: 30\n  acquisition: boil\n  beta_default: 1.0\n\noptuna:\n  n_trials: 30\n  search_space:\n    learning_rate: {type: loguniform, low: 1e-4, high: 1e-1}\n    hidden_units:   {type: int,        low: 64,  high: 512}\n    training_iteration_budget: {type: int, low: 1, high: 15}\n    beta: {type: categorical, choices: [0.5, 1.0]}\n\nresults_dir: outputs\ntrial_mode: false", "evaluate_py": "\"\"\"src/evaluate.py\nIndependent evaluation / visualisation stage.  Fetches full histories\nfrom WandB, creates per-run artefacts (JSON + plots) and an aggregated\ncomparison across runs.  **Not** called during training.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy import stats as stat_module  # avoid name clash\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\nfrom .model import build_model\nfrom .preprocess import get_dataloaders\n\n###############################################################################\n#                             utilities                                       #\n###############################################################################\n\ndef _save_json(path: Path, obj) -\u003e None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"w\", encoding=\"utf-8\") as fh:\n        json.dump(obj, fh, indent=2, sort_keys=False)\n\n\ndef _plot_learning_curve(df, out_dir: Path, run_id: str) -\u003e Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    if \"val_acc\" in df.columns:\n        ax.plot(df[\"val_acc\"], label=\"Validation\", linewidth=2)\n    if \"train_acc\" in df.columns:\n        ax.plot(df[\"train_acc\"], label=\"Training\", alpha=0.5)\n    ax.set_xlabel(\"Logged step\")\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_title(f\"Learning-curve \u2013 {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef _plot_confusion(run_cfg, model_path: Path, out_dir: Path, run_id: str):\n    if not model_path.exists():\n        return None\n\n    import torch\n\n    cfg = OmegaConf.create(run_cfg) if not isinstance(run_cfg, Dict) else run_cfg\n\n    # obtain only **test** loader -------------------------------------------\n    _, _, test_dl = get_dataloaders(cfg.dataset, cfg.training)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = build_model(cfg.model).to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n\n    preds, labels = [], []\n    with torch.no_grad():\n        for x, y in test_dl:\n            x = x.to(device)\n            preds.append(model(x).argmax(1).cpu().numpy())\n            labels.append(y.numpy())\n    y_pred = np.concatenate(preds)\n    y_true = np.concatenate(labels)\n\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(cm)\n    disp.plot(cmap=\"Blues\")\n    plt.title(f\"Confusion matrix \u2013 {run_id}\")\n    plt.tight_layout()\n    path = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    plt.savefig(path)\n    plt.close()\n    return path\n\n###############################################################################\n#                                main                                         #\n###############################################################################\n\ndef main() -\u003e None:  # noqa: C901 \u2013 evaluation requires several steps\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory that contains run sub-folders\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list, e.g. \u0027[", "main_py": "\"\"\"src/main.py\nTop-level orchestrator.  Simply forwards to ``src.train`` so that Hydra\nconfig scopes stay cleanly separated.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -\u003e None:\n    overrides: List[str] = [\n        f\"run={cfg.run.run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n    ]\n    if cfg.trial_mode:\n        overrides.append(\"trial_mode=true\")\n        overrides.append(\"wandb.mode=disabled\")\n    else:\n        overrides.append(f\"wandb.mode={cfg.wandb.mode}\")\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\", *overrides]\n    print(\"[main] launching:\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nNeural-network architectures used in the experiments.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any\n\nimport torch.nn as nn\nfrom omegaconf import DictConfig\n\n\nclass TwoLayerMLP(nn.Module):\n    \"\"\"Tiny 2-layer MLP (~1.2 M params for 256 hidden units).\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_units: int,\n        output_classes: int,\n        activation: str = \"relu\",\n        weight_init: str = \"kaiming_uniform\",\n    ) -\u003e None:\n        super().__init__()\n        act_cls: Any = getattr(nn, activation.capitalize(), nn.ReLU)\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_dim, hidden_units),\n            act_cls(),\n            nn.Linear(hidden_units, output_classes),\n        )\n        self.apply(lambda m: self._init(m, weight_init))\n\n    @staticmethod\n    def _init(m: nn.Module, scheme: str):\n        if isinstance(m, nn.Linear):\n            if scheme == \"kaiming_uniform\":\n                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n            elif scheme == \"kaiming_normal\":\n                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n            elif scheme == \"xavier_uniform\":\n                nn.init.xavier_uniform_(m.weight)\n            elif scheme == \"xavier_normal\":\n                nn.init.xavier_normal_(m.weight)\n            else:\n                raise ValueError(scheme)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n\n    def forward(self, x):  # noqa: D401 \u2013 simple forward\n        return self.net(x)\n\n\ndef build_model(cfg: DictConfig):\n    name = cfg.name.lower()\n    if name == \"two-layer-mlp-1.2m\":\n        return TwoLayerMLP(\n            input_dim=cfg.input_dim,\n            hidden_units=cfg.hidden_units,\n            output_classes=cfg.output_classes,\n            activation=cfg.activation,\n            weight_init=cfg.weight_init,\n        )\n    raise NotImplementedError(cfg.name)\n", "preprocess_py": "\"\"\"src/preprocess.py\nData-loading utilities.  Provides get_dataloaders() for Fashion-MNIST.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import FashionMNIST\n\n###############################################################################\n#                          transform helpers                                  #\n###############################################################################\n\ndef _parse_transforms(entries):\n    tfs = []\n    for e in entries:\n        if isinstance(e, str):\n            e_lc = e.lower()\n            if e_lc == \"totensor\":\n                tfs.append(T.ToTensor())\n            elif e_lc.startswith(\"normalize\"):\n                mean, std = 0.5, 0.5\n                if \"(\" in e and \")\" in e:\n                    inner = e[e.find(\"(\") + 1 : e.find(\")\")]\n                    parts = {k.strip(): float(v) for k, v in (p.split(\"=\") for p in inner.split(\",\"))}\n                    mean, std = parts.get(\"mean\", 0.5), parts.get(\"std\", 0.5)\n                tfs.append(T.Normalize(mean=[mean], std=[std]))\n            else:\n                raise ValueError(e)\n        elif isinstance(e, dict):\n            if \"Normalize\" in e:\n                m = e[\"Normalize\"].get(\"mean\", 0.5)\n                s = e[\"Normalize\"].get(\"std\", 0.5)\n                tfs.append(T.Normalize(mean=[m], std=[s]))\n            else:\n                raise ValueError(e)\n        else:\n            raise TypeError(type(e))\n    return T.Compose(tfs)\n\n###############################################################################\n#                           public interface                                  #\n###############################################################################\n\ndef get_dataloaders(ds_cfg: DictConfig, tr_cfg: DictConfig) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    if ds_cfg.name.lower() != \"fashion-mnist\":\n        raise NotImplementedError(\"Only Fashion-MNIST supported.\")\n\n    tf = _parse_transforms(ds_cfg.transforms)\n    root = \".cache/torchvision/fashion_mnist\"\n\n    train_full = FashionMNIST(root=root, train=True, download=True, transform=tf)\n    test_set = FashionMNIST(root=root, train=False, download=True, transform=tf)\n\n    val_sz = int(len(train_full) * ds_cfg.validation_split)\n    train_sz = len(train_full) - val_sz\n    train_set, val_set = random_split(train_full, [train_sz, val_sz])\n\n    kwargs = dict(batch_size=tr_cfg.batch_size, num_workers=tr_cfg.num_workers, pin_memory=tr_cfg.pin_memory)\n    train_loader = DataLoader(train_set, shuffle=True, **kwargs)\n    val_loader = DataLoader(val_set, shuffle=False, **kwargs)\n    test_loader = DataLoader(test_set, shuffle=False, **kwargs)\n\n    return train_loader, val_loader, test_loader\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"s-boil-experiment\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n  \"torch\u003e=2.0\",\n  \"torchvision\u003e=0.15\",\n  \"hydra-core\u003e=1.3\",\n  \"wandb\u003e=0.15\",\n  \"botorch\u003e=0.8.5\",\n  \"gpytorch\u003e=1.11\",\n  \"optuna\u003e=3.4\",\n  \"numpy\u003e=1.24\",\n  \"pandas\u003e=2.0\",\n  \"matplotlib\u003e=3.7\",\n  \"seaborn\u003e=0.12\",\n  \"scikit-learn\u003e=1.3\",\n  \"scipy\u003e=1.10\"\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "\"\"\"src/train.py\nFull training / optimisation loop for a *single* run-ID.  Implements\nBOIL and S-BOIL exactly as described in the paper and logs **all**\nmetrics to WandB.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport time\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport hydra\nimport numpy as np\nimport torch\nimport wandb\nfrom botorch.fit import fit_gpytorch_model\nfrom botorch.models import SingleTaskGP\nfrom gpytorch.mlls import ExactMarginalLogLikelihood\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.linear_model import LinearRegression\n\nfrom .model import build_model\nfrom .preprocess import get_dataloaders\n\n###############################################################################\n#                           Sampling helpers                                  #\n###############################################################################\n\ndef _log_uniform(low: float, high: float, rng: np.random.Generator) -\u003e float:\n    return float(10 ** rng.uniform(np.log10(low), np.log10(high)))\n\n\ndef _sample(space: Dict, rng: np.random.Generator) -\u003e Dict:\n    out: Dict = {}\n    for name, spec in space.items():\n        t = spec[\"type\"].lower()\n        if t == \"loguniform\":\n            out[name] = _log_uniform(spec[\"low\"], spec[\"high\"], rng)\n        elif t == \"uniform\":\n            out[name] = float(rng.uniform(spec[\"low\"], spec[\"high\"]))\n        elif t == \"int\":\n            out[name] = int(rng.integers(spec[\"low\"], spec[\"high\"] + 1))\n        elif t == \"categorical\":\n            out[name] = rng.choice(spec[\"choices\"])\n        else:\n            raise ValueError(t)\n    return out\n\n\ndef _encode(sample: Dict, space: Dict) -\u003e List[float]:\n    vec: List[float] = []\n    for name, spec in space.items():\n        val = sample[name]\n        t = spec[\"type\"].lower()\n        if t == \"loguniform\":\n            low, high = math.log10(spec[\"low\"]), math.log10(spec[\"high\"])\n            vec.append((math.log10(val) - low) / (high - low))\n        elif t in {\"uniform\", \"int\"}:\n            low, high = spec[\"low\"], spec[\"high\"]\n            vec.append((float(val) - low) / (high - low))\n        elif t == \"categorical\":\n            idx = spec[\"choices\"].index(val)\n            vec.append(idx / (len(spec[\"choices\"]) - 1))\n    return vec\n\n###############################################################################\n#                         One training episode                                #\n###############################################################################\n\ndef _train_episode(\n    sample: Dict,\n    cfg: DictConfig,\n    device: torch.device,\n    trial_mode: bool,\n    wb_run: wandb.sdk.wandb_run.Run,\n    step_offset: int,\n) -\u003e Tuple[float, float, Dict]:\n    \"\"\"Train an MLP for *one* Opt step.  Returns best val-acc, wall-time and\n    best state_dict.\n    \"\"\"\n    t0 = time.perf_counter()\n\n    # merge sampled hyper-parameters into a copy of cfg -----------------------\n    cfg_l = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n    cfg_l.model.hidden_units = int(sample[\"hidden_units\"])\n    cfg_l.training.learning_rate = float(sample[\"learning_rate\"])\n    budget = int(sample[\"training_iteration_budget\"])\n    epochs = 1 if trial_mode else budget * cfg.training.epochs_per_iteration\n\n    train_dl, val_dl, _ = get_dataloaders(cfg.dataset, cfg.training)\n\n    model = build_model(cfg_l.model).to(device)\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg_l.training.learning_rate)\n\n    best_val, best_state = 0.0, {}\n    global_step = step_offset\n\n    for epoch in range(1, epochs + 1):\n        # ----------------------------- train -------------------------------\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n        for b_idx, (x, y) in enumerate(train_dl, 1):\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * y.size(0)\n            correct += (logits.argmax(1) == y).sum().item()\n            total += y.size(0)\n\n            wb_run.log({\"batch_loss\": loss.item(), \"phase\": \"train\"}, step=global_step)\n            global_step += 1\n            if trial_mode and b_idx \u003e= 2:\n                break\n\n        train_loss = running_loss / max(total, 1)\n        train_acc = correct / max(total, 1)\n\n        # --------------------------- validation ---------------------------\n        model.eval()\n        v_loss, v_corr, v_tot = 0.0, 0, 0\n        with torch.no_grad():\n            for x, y in val_dl:\n                x, y = x.to(device), y.to(device)\n                logits = model(x)\n                loss = criterion(logits, y)\n                v_loss += loss.item() * y.size(0)\n                v_corr += (logits.argmax(1) == y).sum().item()\n                v_tot += y.size(0)\n        val_loss = v_loss / max(v_tot, 1)\n        val_acc = v_corr / max(v_tot, 1)\n\n        if val_acc \u003e best_val:\n            best_val = val_acc\n            best_state = deepcopy(model.state_dict())\n\n        wb_run.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"hyper/learning_rate\": cfg_l.training.learning_rate,\n            \"hyper/hidden_units\": cfg_l.model.hidden_units,\n            \"hyper/epochs\": epochs,\n        }, step=global_step)\n\n        if trial_mode:\n            break\n\n    wall = time.perf_counter() - t0\n    return best_val, wall, best_state\n\n###############################################################################\n#                                   Main                                      #\n###############################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -\u003e None:  # noqa: C901\n    # ------------------ trial-mode adjustments ------------------------------\n    if cfg.trial_mode:\n        cfg.wandb.mode = \"disabled\"\n        cfg.experiment.total_evaluations = 1\n        cfg.experiment.n_init_points = 1\n        cfg.training.epochs_per_iteration = 1\n        cfg.optuna.n_trials = 0\n\n    out_dir = Path(hydra.utils.to_absolute_path(cfg.results_dir)) / cfg.run.run_id\n    out_dir.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, out_dir / \"config.yaml\")\n\n    wb_run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run.run_id,\n        dir=str(out_dir),\n        mode=cfg.wandb.mode,\n        resume=\"allow\",\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    if wb_run.mode != \"disabled\":\n        print(f\"[WandB] URL: {wb_run.url}\")\n\n    device = torch.device(cfg.training.device if torch.cuda.is_available() else \"cpu\")\n\n    rng = np.random.default_rng(seed=42)\n    space = cfg.optuna.search_space\n\n    X, Y, C, samples = [], [], [], []\n    best_global, best_state = -1.0, {}\n\n    for ev in range(cfg.experiment.total_evaluations):\n        # ------------------------- suggest ---------------------------------\n        if ev \u003c cfg.experiment.n_init_points:\n            candidate = _sample(space, rng)\n        else:\n            X_t = torch.tensor(X, dtype=torch.double)\n            Y_t = torch.tensor(Y, dtype=torch.double).unsqueeze(-1)\n            gp = SingleTaskGP(X_t, Y_t)\n            mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n            fit_gpytorch_model(mll)\n            best_y = max(Y)\n\n            cost_model = LinearRegression().fit(np.array(X), np.log(np.array(C)))\n\n            pool = [_sample(space, rng) for _ in range(512)]\n            X_pool = torch.tensor([_encode(p, space) for p in pool], dtype=torch.double)\n            post = gp.posterior(X_pool)\n            mu, var = post.mean.squeeze(-1), post.variance.squeeze(-1)\n            sigma = var.clamp_min(1e-12).sqrt()\n            z = (mu - best_y) / sigma\n            norm = torch.distributions.Normal(torch.zeros_like(z), torch.ones_like(z))\n            ei = (mu - best_y) * norm.cdf(z) + sigma * norm.log_prob(z).exp()\n            ei = ei.clamp_min(1e-12)\n\n            cost_pred = torch.from_numpy(np.exp(cost_model.predict(X_pool.cpu().numpy()))).double()\n\n            if cfg.experiment.acquisition.lower() == \"boil\":\n                acq = torch.log(ei) - torch.log(cost_pred)\n            elif cfg.experiment.acquisition.lower() == \"s-boil\":\n                betas = torch.tensor([p.get(\"beta\", cfg.experiment.beta_default) for p in pool], dtype=torch.double)\n                acq = torch.log(ei) - betas * var - torch.log(cost_pred)\n            else:\n                raise ValueError(cfg.experiment.acquisition)\n            candidate = pool[int(torch.argmax(acq).item())]\n\n        wb_run.log({f\"suggested/{k}\": v for k, v in candidate.items()}, step=ev)\n\n        v_acc, w_time, best_loc = _train_episode(\n            candidate, cfg, device, cfg.trial_mode, wb_run, step_offset=ev * 10_000\n        )\n        X.append(_encode(candidate, space))\n        Y.append(v_acc)\n        C.append(w_time)\n        samples.append(candidate)\n\n        if v_acc \u003e best_global:\n            best_global = v_acc\n            best_state = deepcopy(best_loc)\n            wb_run.summary[\"best_val_accuracy\"] = best_global\n\n        wb_run.log({\n            \"evaluation_id\": ev,\n            \"val_acc\": v_acc,\n            \"wall_time\": w_time,\n            \"best_val_acc_so_far\": best_global,\n        }, step=(ev + 1) * 10_000)\n\n        if v_acc \u003e= 0.8 and \"time_to_target\" not in wb_run.summary:\n            wb_run.summary[\"time_to_target\"] = float(sum(C))\n            wb_run.summary[\"evals_to_target\"] = ev + 1\n\n        if cfg.trial_mode:\n            break\n\n    wb_run.summary[\"total_wall_time\"] = float(sum(C))\n\n    if best_state:\n        torch.save(best_state, out_dir / \"best_model.pt\")\n        wb_run.save(str(out_dir / \"best_model.pt\"), base_path=str(out_dir))\n\n    wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "In BOIL, the acquisition score is obtained by dividing Expected-Improvement (EI) by an estimated cost coming from a simple linear regression.  However, EI is computed only from the GP predictive mean; it completely ignores the predictive variance of the final performance that the GP supplies.  As a consequence BOIL often proposes configurations whose *expected* gain is large but whose *uncertainty* is also large, leading to wasted wall-clock time on unstable or highly noisy runs.  A very small change that explicitly trades off improvement against predictive risk can reduce these wasted evaluations without touching the GP model, the curve-compression, or the selective data-augmentation machinery.",
    "Methods": "Stability-Aware Cost-Efficient BOIL (S-BOIL)\nMinimal change: multiply the utility part of BOIL’s cost-aware acquisition by an exponential penalty that depends on the GP’s predictive variance σ²(x,t) at the candidate (x,t):\n\n    Utility_s = EI(x,t) * exp(-β · σ²(x,t))\n    Acquisition  =  log(Utility_s)  –  log(EstimatedCost+0.1)\n\nwhere β≥0 is a single tunable constant (β≈1 works well).  Intuition: for equal EI, prefer points the GP is certain about; for equal variance, recover the original BOIL rule.  Because σ² is already returned by the GP, the only code change is three lines in the acquisition function.  No retraining or extra hyper-parameters are introduced apart from β.",
    "Experimental Setup": "Base method: original BOIL implementation.\nProposed: S-BOIL (β ∈ {0.5,1}).\nTask: tune two hyper-parameters (learning-rate, hidden-units) and the training-iteration budget for an MLP on the Fashion-MNIST 10-class task (using PyTorch).  Each BO run is allowed 30 real evaluations; wall-clock time is logged.\nMetrics:\n1. Best validation accuracy reached vs. number of real evaluations.\n2. Total wall-clock time until a target accuracy (80%) is hit.\nProtocol: 10 independent seeds for each method.  Same initial random design (3 points).",
    "Experimental Code": "# --- patch to BOIL: Stability penalty ------------------------------------\n# in utility_cost_evaluation_single() *replace* the EI computation line\n# original: utility = acq_func.acq_kind(x,gp=self.gp)\n# new lines:\nmean, var = self.gp.predict(x.reshape(1,-1), return_var=True)\nei_val  = acq_func.ei(mean, var, acq_func.best_y)   # unchanged helper\nutility = ei_val * np.exp(-beta * var)              # <- stability term\n# everything else stays the same\n# ----------------------------------------------------------\n\n# Minimal runnable demo (requires original BOIL code in path)\nimport torch, torchvision, torch.nn as nn, torch.optim as optim\nfrom boil import BOIL        # assume original class name\n\nbeta = 1.0  # stability weight\nbo = BOIL(func=mlp_runner, SearchSpace={\"lr\":(1e-4,1e-1),\n                                        \"units\":(64,512),\n                                        \"T\":(1,15)})\nbo.beta = beta               # pass to acquisition helper\nbo.init(n_init_points=3)\nfor _ in range(27):          # 30 evaluations total\n    bo.suggest_nextpoint()\nprint(\"Best acc =\", bo.Y_original.max())",
    "Expected Result": "Across 10 seeds S-BOIL is expected to reach 80% validation accuracy in ~15% less wall-clock time and 2 fewer function evaluations on average compared with BOIL.  The area-under-curve of best-seen accuracy vs. evaluations should also improve by ≈5–7%.",
    "Expected Conclusion": "A tiny, one-line modification that penalizes predictive variance lets BOIL avoid expensive yet uncertain configurations, giving faster convergence at virtually no additional computational cost or implementation complexity.  This demonstrates that explicitly accounting for GP uncertainty inside the acquisition utility—beyond its usual role in EI—is a simple but powerful way to improve efficiency in iterative hyperparameter optimization."
}

# Experimental Design
- Summary: The experiment aims to verify that the Stability-Aware Cost-Efficient BOIL (S-BOIL) acquisition rule accelerates hyper-parameter tuning compared with the original BOIL.  We run black-box optimisation on a two-layer MLP (≈1.2 M parameters) that is trained on Fashion-MNIST.  The optimiser must choose three configuration variables—learning-rate, hidden-units, and training-iteration budget—over 30 real training runs.  For each of 10 random seeds we execute (1) vanilla BOIL and (2) S-BOIL with a variance-penalised utility EI·exp(−βσ²).  β is treated as a small discrete hyper-parameter {0.5,1}.  All other code, GP surrogate, cost model, and initial design remain identical.  Metrics recorded at every function evaluation are: best-seen validation accuracy, wall-clock time, and the area-under-curve (AUC) of accuracy versus evaluations.  The primary comparisons are (i) mean evaluations and time to reach 80 % accuracy and (ii) mean AUAC across seeds.  The whole study fits comfortably on one A100 GPU, but eight are available if parallel seeds are desired.
- Evaluation metrics: ['Validation Accuracy', 'Wall-clock Time to Target', 'Area Under Accuracy Curve']

# Experiment Runs

- Run ID: proposed-Two-Layer-MLP-1.2M-Fashion-MNIST
  Method: proposed
  Model: Two-Layer-MLP-1.2M
  Dataset: Fashion-MNIST
  Config File: config/run/proposed-Two-Layer-MLP-1.2M-Fashion-MNIST.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-Two-Layer-MLP-1.2M-Fashion-MNIST
method: S-BOIL
model:
  name: Two-Layer-MLP-1.2M
  input_dim: 784
  hidden_units: 256        # default, tuned via Optuna
  output_classes: 10
  activation: relu
  weight_init: kaiming_uniform
dataset:
  name: Fashion-MNIST
  transforms:
    - ToTensor
    - Normalize(mean=0.5, std=0.5)
  validation_split: 0.1
training:
  batch_size: 128
  optimizer: adam
  loss_fn: cross_entropy
  device: cuda
  default_learning_rate: 1e-3
  epochs_per_iteration: 5   # total epochs = budget * 5
  num_workers: 8
  pin_memory: true
experiment:
  n_init_points: 3
  total_evaluations: 30
  acquisition: S-BOIL       # log(EI*exp(-βσ²)) − log(cost)
  beta_default: 1.0
optuna:
  n_trials: 30
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    hidden_units:
      type: int
      low: 64
      high: 512
    training_iteration_budget:
      type: int
      low: 1
      high: 15
    beta:
      type: categorical
      choices: [0.5, 1.0]

  ```
  

- Run ID: comparative-1-Two-Layer-MLP-1.2M-Fashion-MNIST
  Method: comparative-1
  Model: Two-Layer-MLP-1.2M
  Dataset: Fashion-MNIST
  Config File: config/run/comparative-1-Two-Layer-MLP-1.2M-Fashion-MNIST.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-Two-Layer-MLP-1.2M-Fashion-MNIST
method: BOIL
model:
  name: Two-Layer-MLP-1.2M
  input_dim: 784
  hidden_units: 256        # default, tuned via Optuna
  output_classes: 10
  activation: relu
  weight_init: kaiming_uniform
dataset:
  name: Fashion-MNIST
  transforms:
    - ToTensor
    - Normalize(mean=0.5, std=0.5)
  validation_split: 0.1
training:
  batch_size: 128
  optimizer: adam
  loss_fn: cross_entropy
  device: cuda
  default_learning_rate: 1e-3
  epochs_per_iteration: 5
  num_workers: 8
  pin_memory: true
experiment:
  n_init_points: 3
  total_evaluations: 30
  acquisition: BOIL         # log(EI) − log(cost)
optuna:
  n_trials: 30
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    hidden_units:
      type: int
      low: 64
      high: 512
    training_iteration_budget:
      type: int
      low: 1
      high: 15

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**


**HuggingFace Datasets:**

- ID: zalando-datasets/fashion_mnist




Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "\"\"\"src/train.py\nFull training / optimisation loop for ONE run-id. Implements vanilla\nBOIL and the proposed Stability-aware BOIL (S-BOIL).  Everything is\nfully logged to WandB – absolutely no local JSON summaries are\nproduced.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport time\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport hydra\nimport numpy as np\nimport torch\nimport wandb\nfrom botorch.fit import fit_gpytorch_model\nfrom botorch.models import SingleTaskGP\nfrom gpytorch.mlls import ExactMarginalLogLikelihood\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.linear_model import LinearRegression\n\nfrom .model import build_model\nfrom .preprocess import get_dataloaders\n\n################################################################################\n#                        Helper : sampling search-space                        #\n################################################################################\n\ndef _log_uniform(low: float, high: float, rng: np.random.Generator) -> float:\n    \"\"\"Sample from a base-10 log-uniform distribution.\"\"\"\n    return float(10 ** rng.uniform(np.log10(low), np.log10(high)))\n\n\ndef _sample(space: Dict, rng: np.random.Generator) -> Dict:\n    \"\"\"Draw *one* random sample from an Optuna-style search-space dict.\"\"\"\n    out: Dict = {}\n    for name, spec in space.items():\n        t = spec[\"type\"].lower()\n        if t == \"loguniform\":\n            out[name] = _log_uniform(spec[\"low\"], spec[\"high\"], rng)\n        elif t == \"uniform\":\n            out[name] = float(rng.uniform(spec[\"low\"], spec[\"high\"]))\n        elif t == \"int\":\n            out[name] = int(rng.integers(spec[\"low\"], spec[\"high\"] + 1))\n        elif t == \"categorical\":\n            out[name] = rng.choice(spec[\"choices\"])\n        else:\n            raise ValueError(f\"Unknown space type: {t}\")\n    return out\n\n\ndef _encode(sample: Dict, space: Dict) -> List[float]:\n    \"\"\"Normalise a sample to [0,1] – required by the GP surrogate.\"\"\"\n    vec: List[float] = []\n    for name, spec in space.items():\n        val = sample[name]\n        t = spec[\"type\"].lower()\n        if t == \"loguniform\":\n            low, high = math.log10(spec[\"low\"]), math.log10(spec[\"high\"])\n            vec.append((math.log10(val) - low) / (high - low))\n        elif t in {\"uniform\", \"int\"}:\n            low, high = spec[\"low\"], spec[\"high\"]\n            vec.append((float(val) - low) / (high - low))\n        elif t == \"categorical\":\n            idx = spec[\"choices\"].index(val)\n            vec.append(idx / (len(spec[\"choices\"]) - 1))\n    return vec\n\n################################################################################\n#                      Single training episode (inner loop)                    #\n################################################################################\n\ndef _train_episode(\n    sample: Dict,\n    cfg: DictConfig,\n    device: torch.device,\n    trial_mode: bool,\n    wb_run: wandb.sdk.wandb_run.Run,\n    step_offset: int,\n) -> Tuple[float, float, Dict]:\n    \"\"\"Trains an MLP with the *sampled* hyper-parameters.\n\n    Returns\n    -------\n    best_val : float\n        Best validation accuracy achieved in the episode.\n    wall_time : float\n        Wall-clock time consumed (in seconds).\n    best_state : Dict\n        ``state_dict`` of the best model (empty dict if nothing improved).\n    \"\"\"\n    t0 = time.perf_counter()\n\n    # ------------------------------------------------------------------ merge\n    # We create a *local* copy of the configuration into which the sampled\n    # values are injected.  This leaves the original cfg untouched and safe\n    # for the following episodes.\n    cfg_l = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n    cfg_l.model.hidden_units = int(sample[\"hidden_units\"])\n    cfg_l.training.learning_rate = float(sample[\"learning_rate\"])\n    budget = int(sample[\"training_iteration_budget\"])\n    epochs = 1 if trial_mode else budget * cfg.training.epochs_per_iteration\n\n    train_dl, val_dl, _ = get_dataloaders(cfg.dataset, cfg.training)\n\n    model = build_model(cfg_l.model).to(device)\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg_l.training.learning_rate)\n\n    best_val, best_state = 0.0, {}\n    global_step = step_offset\n\n    for epoch in range(1, epochs + 1):\n        # --------------------------- training loop -----------------------\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n        for b_idx, (x, y) in enumerate(train_dl, 1):\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            logits = model(x)\n            loss = criterion(logits, y)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * y.size(0)\n            correct += (logits.argmax(1) == y).sum().item()\n            total += y.size(0)\n\n            wb_run.log({\"batch_loss\": loss.item(), \"phase\": \"train\"}, step=global_step)\n            global_step += 1\n            if trial_mode and b_idx >= 2:\n                break\n\n        train_loss = running_loss / max(total, 1)\n        train_acc = correct / max(total, 1)\n\n        # --------------------------- validation -------------------------\n        model.eval()\n        v_loss, v_corr, v_tot = 0.0, 0, 0\n        with torch.no_grad():\n            for x, y in val_dl:\n                x, y = x.to(device), y.to(device)\n                logits = model(x)\n                loss = criterion(logits, y)\n                v_loss += loss.item() * y.size(0)\n                v_corr += (logits.argmax(1) == y).sum().item()\n                v_tot += y.size(0)\n        val_loss = v_loss / max(v_tot, 1)\n        val_acc = v_corr / max(v_tot, 1)\n\n        if val_acc > best_val:\n            best_val = val_acc\n            best_state = deepcopy(model.state_dict())\n\n        wb_run.log(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n                \"hyper/learning_rate\": cfg_l.training.learning_rate,\n                \"hyper/hidden_units\": cfg_l.model.hidden_units,\n                \"hyper/epochs\": epochs,\n            },\n            step=global_step,\n        )\n\n        if trial_mode:\n            break\n\n    wall = time.perf_counter() - t0\n    return best_val, wall, best_state\n\n################################################################################\n#                                    Main                                      #\n################################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -> None:  # noqa: C901 – function is inevitably long\n    # ---------------------- trial-mode soft overrides -------------------------\n    if cfg.trial_mode:\n        cfg.wandb.mode = \"disabled\"\n        cfg.experiment.total_evaluations = 1\n        cfg.experiment.n_init_points = 1\n        cfg.training.epochs_per_iteration = 1\n        cfg.optuna.n_trials = 0\n\n    # ----------------------------- output dirs --------------------------------\n    root_dir = Path(hydra.utils.to_absolute_path(cfg.results_dir))\n    run_dir = root_dir / cfg.run.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save *this* run's resolved configuration for full reproducibility\n    OmegaConf.save(cfg, run_dir / \"config.yaml\")\n\n    # Also place a global WandB pointer file at <results_dir>/config.yaml so\n    # that the evaluation stage can easily recover entity/project details.\n    global_cfg_path = root_dir / \"config.yaml\"\n    if not global_cfg_path.exists():\n        global_wb_cfg = OmegaConf.create({\n            \"wandb\": {\"entity\": cfg.wandb.entity, \"project\": cfg.wandb.project}\n        })\n        OmegaConf.save(global_wb_cfg, global_cfg_path)\n\n    # ------------------------------ WandB -------------------------------------\n    wb_run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run.run_id,\n        dir=str(run_dir),\n        mode=cfg.wandb.mode,\n        resume=\"allow\",\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    if wb_run.mode != \"disabled\":\n        print(f\"[WandB] run URL: {wb_run.url}\")\n\n    device = torch.device(cfg.training.device if torch.cuda.is_available() else \"cpu\")\n    rng = np.random.default_rng(seed=42)\n    space = cfg.optuna.search_space\n\n    # ------------------- containers for BOIL / S-BOIL -------------------------\n    X, Y, C, samples = [], [], [], []  # design-matrix, targets, costs, raw-samples\n    best_global, best_state = -1.0, {}\n\n    for ev in range(cfg.experiment.total_evaluations):\n        # ------------------------ suggestion phase -------------------------\n        if ev < cfg.experiment.n_init_points:\n            candidate = _sample(space, rng)\n        else:\n            # Build GP on *normalised* X, Y\n            X_t = torch.tensor(X, dtype=torch.double)\n            Y_t = torch.tensor(Y, dtype=torch.double).unsqueeze(-1)\n            gp = SingleTaskGP(X_t, Y_t)\n            mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n            fit_gpytorch_model(mll)\n            best_y = max(Y)\n\n            # Cheap linear regression surrogate for cost\n            cost_model = LinearRegression().fit(np.array(X), np.log(np.array(C)))\n\n            # Generate a candidate pool\n            pool = [_sample(space, rng) for _ in range(512)]\n            X_pool = torch.tensor([_encode(p, space) for p in pool], dtype=torch.double)\n\n            post = gp.posterior(X_pool)\n            mu, var = post.mean.squeeze(-1), post.variance.squeeze(-1).clamp_min(1e-12)\n            sigma = var.sqrt()\n            z = (mu - best_y) / sigma\n            norm = torch.distributions.Normal(torch.zeros_like(z), torch.ones_like(z))\n            ei = (mu - best_y) * norm.cdf(z) + sigma * norm.log_prob(z).exp()\n            ei = ei.clamp_min(1e-12)\n\n            cost_pred = torch.from_numpy(np.exp(cost_model.predict(X_pool.cpu().numpy()))).double()\n\n            acquis = cfg.experiment.acquisition.lower()\n            if acquis == \"boil\":\n                acq = torch.log(ei) - torch.log(cost_pred)\n            elif acquis == \"s-boil\":\n                betas = torch.tensor([\n                    p.get(\"beta\", cfg.experiment.beta_default) for p in pool\n                ], dtype=torch.double)\n                acq = torch.log(ei) - betas * var - torch.log(cost_pred)\n            else:\n                raise ValueError(f\"Unknown acquisition: {cfg.experiment.acquisition}\")\n\n            candidate = pool[int(torch.argmax(acq).item())]\n\n        wb_run.log({f\"suggested/{k}\": v for k, v in candidate.items()}, step=ev)\n\n        # -------------------- objective (training episode) ------------------\n        v_acc, w_time, best_loc = _train_episode(\n            candidate, cfg, device, cfg.trial_mode, wb_run, step_offset=ev * 10_000\n        )\n\n        X.append(_encode(candidate, space))\n        Y.append(v_acc)\n        C.append(w_time)\n        samples.append(candidate)\n\n        # ---------------------- bookkeeping --------------------------------\n        if v_acc > best_global:\n            best_global = v_acc\n            best_state = deepcopy(best_loc)\n            wb_run.summary[\"best_val_accuracy\"] = best_global\n\n        wb_run.log({\n            \"evaluation_id\": ev,\n            \"val_acc\": v_acc,\n            \"wall_time\": w_time,\n            \"best_val_acc_so_far\": best_global,\n        }, step=(ev + 1) * 10_000)\n\n        # first time hitting 0.8 accuracy ⇒ record time-to-target\n        if v_acc >= 0.8 and \"time_to_target\" not in wb_run.summary:\n            wb_run.summary[\"time_to_target\"] = float(sum(C))\n            wb_run.summary[\"evals_to_target\"] = ev + 1\n\n        if cfg.trial_mode:\n            break\n\n    wb_run.summary[\"total_wall_time\"] = float(sum(C))\n\n    # ------------------------------ artefacts ---------------------------------\n    if best_state:\n        torch.save(best_state, run_dir / \"best_model.pt\")\n        wb_run.save(str(run_dir / \"best_model.pt\"), base_path=str(run_dir))\n\n    wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nIndependent evaluation / visualisation stage.  Fetches full histories\nfrom WandB, creates per-run artefacts (JSON + plots) and an aggregated\ncomparison across runs.  **Not** called during training.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy import stats as sstats\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\nfrom .model import build_model\nfrom .preprocess import get_dataloaders\n\n################################################################################\n#                               utilities                                      #\n################################################################################\n\ndef _save_json(path: Path, obj) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"w\", encoding=\"utf-8\") as fh:\n        json.dump(obj, fh, indent=2, sort_keys=False)\n\n\ndef _plot_learning_curve(df: pd.DataFrame, out_dir: Path, run_id: str) -> Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    if \"val_acc\" in df.columns:\n        ax.plot(df[\"val_acc\"], label=\"Validation\", linewidth=2)\n    if \"train_acc\" in df.columns:\n        ax.plot(df[\"train_acc\"], label=\"Training\", alpha=0.5)\n    ax.set_xlabel(\"Logged step\")\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_title(f\"Learning curve – {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef _plot_confusion(run_cfg: Dict, model_path: Path, out_dir: Path, run_id: str):\n    if not model_path.exists():\n        return None\n\n    import torch\n\n    cfg = OmegaConf.create(run_cfg) if not isinstance(run_cfg, Dict) else run_cfg\n\n    # obtain ONLY test loader --------------------------------------------------\n    _, _, test_dl = get_dataloaders(cfg.dataset, cfg.training)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = build_model(cfg.model).to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n\n    preds, labels = [], []\n    with torch.no_grad():\n        for x, y in test_dl:\n            x = x.to(device)\n            preds.append(model(x).argmax(1).cpu().numpy())\n            labels.append(y.numpy())\n    y_pred = np.concatenate(preds)\n    y_true = np.concatenate(labels)\n\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(cm)\n    disp.plot(cmap=\"Blues\")\n    plt.title(f\"Confusion matrix – {run_id}\")\n    plt.tight_layout()\n    path = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    plt.savefig(path)\n    plt.close()\n    return path\n\n################################################################################\n#                               main                                           #\n################################################################################\n\ndef _load_global_wandb_cfg(results_dir: Path) -> Tuple[str, str]:\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(\n            f\"Could not locate global config at {cfg_path}. Each training run \"\n            \"should have created it.\"\n        )\n    cfg = OmegaConf.load(cfg_path)\n    return cfg.wandb.entity, cfg.wandb.project\n\n\ndef main() -> None:  # noqa: C901 – evaluation is necessarily multi-step\n    parser = argparse.ArgumentParser(description=\"Comprehensive evaluation script\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory that contains run sub-folders\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list, e.g. '[\\\"run-1\\\", \\\"run-2\\\"]'\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    entity, project = _load_global_wandb_cfg(results_dir)\n    api = wandb.Api()\n\n    aggregated_metrics: Dict[str, Dict] = {}\n    generated_paths: List[str] = []\n\n    # ---------------------------------------------------------------- per-run\n    for run_id in run_ids:\n        run_dir = results_dir / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        run = api.run(f\"{entity}/{project}/{run_id}\")\n        history_df = run.history()  # pandas DataFrame\n        summary = dict(run.summary)  # final metrics\n        config = dict(run.config)\n\n        # Save metrics --------------------------------------------------------\n        metrics_out = {\n            \"history\": history_df.to_dict(orient=\"list\"),\n            \"summary\": summary,\n            \"config\": config,\n        }\n        metrics_path = run_dir / \"metrics.json\"\n        _save_json(metrics_path, metrics_out)\n        generated_paths.append(str(metrics_path))\n\n        # Learning curve ------------------------------------------------------\n        lc_path = _plot_learning_curve(history_df, run_dir, run_id)\n        generated_paths.append(str(lc_path))\n\n        # Confusion matrix (if model available) -------------------------------\n        best_model_path = run_dir / \"best_model.pt\"\n        cm_path = _plot_confusion(config, best_model_path, run_dir, run_id)\n        if cm_path is not None:\n            generated_paths.append(str(cm_path))\n\n        # Register for aggregation -------------------------------------------\n        aggregated_metrics[run_id] = {\n            \"best_val_accuracy\": summary.get(\"best_val_accuracy\", float(\"nan\")),\n            \"total_wall_time\": summary.get(\"total_wall_time\", float(\"nan\")),\n            \"time_to_target\": summary.get(\"time_to_target\", float(\"nan\")),\n            \"evals_to_target\": summary.get(\"evals_to_target\", float(\"nan\")),\n        }\n\n    # --------------------------------------------------------- aggregated step\n    cmp_dir = results_dir / \"comparison\"\n    cmp_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save aggregated metrics JSON -------------------------------------------\n    agg_metrics_path = cmp_dir / \"aggregated_metrics.json\"\n    _save_json(agg_metrics_path, aggregated_metrics)\n    generated_paths.append(str(agg_metrics_path))\n\n    # Improvement-rate vs. baseline (take first run as baseline) -------------\n    baseline_id = run_ids[0]\n    baseline_acc = aggregated_metrics[baseline_id][\"best_val_accuracy\"]\n\n    improvement_rates = {\n        rid: (m[\"best_val_accuracy\"] - baseline_acc) / baseline_acc if not np.isnan(baseline_acc) else np.nan\n        for rid, m in aggregated_metrics.items()\n    }\n\n    # Bar chart of best validation accuracy ----------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(x=list(aggregated_metrics.keys()), y=[m[\"best_val_accuracy\"] for m in aggregated_metrics.values()], ax=ax)\n    ax.set_ylabel(\"Best validation accuracy\")\n    ax.set_xlabel(\"Run ID\")\n    for p, acc in zip(ax.patches, [m[\"best_val_accuracy\"] for m in aggregated_metrics.values()]):\n        ax.annotate(f\"{acc:.3f}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                    ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    bar_path = cmp_dir / \"comparison_best_val_accuracy_bar_chart.pdf\"\n    plt.savefig(bar_path)\n    plt.close()\n    generated_paths.append(str(bar_path))\n\n    # Box plot of improvement rates -----------------------------------------\n    if len(run_ids) > 1 and not np.isnan(baseline_acc):\n        fig, ax = plt.subplots(figsize=(6, 4))\n        sns.barplot(x=list(improvement_rates.keys()), y=list(improvement_rates.values()), ax=ax, palette=\"viridis\")\n        ax.set_ylabel(\"Improvement rate vs. baseline\")\n        ax.set_xlabel(\"Run ID\")\n        for p, imp in zip(ax.patches, list(improvement_rates.values())):\n            ax.annotate(f\"{imp:.2%}\", (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                        ha=\"center\", va=\"bottom\", fontsize=8)\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        imp_path = cmp_dir / \"comparison_improvement_rate_bar_chart.pdf\"\n        plt.savefig(imp_path)\n        plt.close()\n        generated_paths.append(str(imp_path))\n\n    # Simple statistical significance: t-test on best accuracies -------------\n    if len(run_ids) > 1:\n        accs = np.array([aggregated_metrics[r][\"best_val_accuracy\"] for r in run_ids])\n        # pair-wise t-test against baseline\n        pvals = {\n            rid: float(sstats.ttest_ind([aggregated_metrics[baseline_id][\"best_val_accuracy\"]],\n                                        [aggregated_metrics[rid][\"best_val_accuracy\"]],\n                                        equal_var=False).pvalue)\n            for rid in run_ids[1:]\n        }\n        pval_path = cmp_dir / \"comparison_pvalues.json\"\n        _save_json(pval_path, pvals)\n        generated_paths.append(str(pval_path))\n\n    # ------------------------------- stdout ----------------------------------\n    print(\"\\n\".join(generated_paths))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nData-loading utilities.  Currently supports the Fashion-MNIST dataset but\nis written so that additional datasets can be added with minimal effort.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import FashionMNIST\n\n################################################################################\n#                         transformation helpers                               #\n################################################################################\n\ndef _parse_transforms(entries):\n    \"\"\"Convert YAML-style transform specifications into torchvision objects.\"\"\"\n    tfs = []\n    for e in entries:\n        if isinstance(e, str):\n            e_lc = e.lower()\n            if e_lc == \"totensor\":\n                tfs.append(T.ToTensor())\n            elif e_lc.startswith(\"normalize\"):\n                # Format: Normalize(mean=0.5, std=0.5)\n                mean, std = 0.5, 0.5\n                if \"(\" in e and \")\" in e:\n                    inner = e[e.find(\"(\") + 1 : e.find(\")\")]\n                    parts = {k.strip(): float(v) for k, v in (p.split(\"=\") for p in inner.split(\",\"))}\n                    mean, std = parts.get(\"mean\", 0.5), parts.get(\"std\", 0.5)\n                tfs.append(T.Normalize(mean=[mean], std=[std]))\n            else:\n                raise ValueError(f\"Unknown transform: {e}\")\n        elif isinstance(e, dict):\n            if \"Normalize\" in e:\n                m = e[\"Normalize\"].get(\"mean\", 0.5)\n                s = e[\"Normalize\"].get(\"std\", 0.5)\n                tfs.append(T.Normalize(mean=[m], std=[s]))\n            else:\n                raise ValueError(f\"Unsupported dict transform: {e}\")\n        else:\n            raise TypeError(f\"Transform must be str or dict, got: {type(e)}\")\n    return T.Compose(tfs)\n\n################################################################################\n#                           public interface                                   #\n################################################################################\n\ndef get_dataloaders(ds_cfg: DictConfig, tr_cfg: DictConfig) -> Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"Return train/val/test dataloaders given Hydra configs.\"\"\"\n    if ds_cfg.name.replace(\"_\", \"-\").lower() != \"fashion-mnist\":\n        raise NotImplementedError(\"Currently only Fashion-MNIST is supported.\")\n\n    tf = _parse_transforms(ds_cfg.transforms)\n    root = \".cache/torchvision/fashion_mnist\"\n\n    train_full = FashionMNIST(root=root, train=True, download=True, transform=tf)\n    test_set = FashionMNIST(root=root, train=False, download=True, transform=tf)\n\n    val_sz = int(len(train_full) * ds_cfg.validation_split)\n    train_sz = len(train_full) - val_sz\n    train_set, val_set = random_split(train_full, [train_sz, val_sz])\n\n    dl_kwargs = dict(\n        batch_size=tr_cfg.batch_size,\n        num_workers=tr_cfg.num_workers,\n        pin_memory=tr_cfg.pin_memory,\n    )\n    train_loader = DataLoader(train_set, shuffle=True, **dl_kwargs)\n    val_loader = DataLoader(val_set, shuffle=False, **dl_kwargs)\n    test_loader = DataLoader(test_set, shuffle=False, **dl_kwargs)\n\n    return train_loader, val_loader, test_loader\n",
    "model_py": "\"\"\"src/model.py\nNeural-network architectures used in the experiments.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any\n\nimport torch.nn as nn\nfrom omegaconf import DictConfig\n\n################################################################################\n#                               architectures                                  #\n################################################################################\n\nclass TwoLayerMLP(nn.Module):\n    \"\"\"Simple two-layer perceptron (~1.2 M parameters for 256 units).\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_units: int,\n        output_classes: int,\n        activation: str = \"relu\",\n        weight_init: str = \"kaiming_uniform\",\n    ) -> None:\n        super().__init__()\n        act_cls: Any = getattr(nn, activation.capitalize(), nn.ReLU)\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_dim, hidden_units),\n            act_cls(),\n            nn.Linear(hidden_units, output_classes),\n        )\n        self.apply(lambda m: self._init(m, weight_init))\n\n    @staticmethod\n    def _init(m: nn.Module, scheme: str):\n        if isinstance(m, nn.Linear):\n            if scheme == \"kaiming_uniform\":\n                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n            elif scheme == \"kaiming_normal\":\n                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n            elif scheme == \"xavier_uniform\":\n                nn.init.xavier_uniform_(m.weight)\n            elif scheme == \"xavier_normal\":\n                nn.init.xavier_normal_(m.weight)\n            else:\n                raise ValueError(f\"Weight-init scheme '{scheme}' not recognised.\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n\n    def forward(self, x):  # noqa: D401 – trivial forward\n        return self.net(x)\n\n################################################################################\n#                               factory                                        #\n################################################################################\n\ndef build_model(cfg: DictConfig):\n    name = cfg.name.lower()\n    if name == \"two-layer-mlp-1.2m\" or name == \"two_layer_mlp_1.2m\":\n        return TwoLayerMLP(\n            input_dim=cfg.input_dim,\n            hidden_units=cfg.hidden_units,\n            output_classes=cfg.output_classes,\n            activation=cfg.activation,\n            weight_init=cfg.weight_init,\n        )\n    raise NotImplementedError(f\"Unknown model: {cfg.name}\")\n",
    "main_py": "\"\"\"src/main.py\nTop-level orchestrator.  Merely forwards to ``src.train`` so that Hydra\nscopes stay cleanly separated.  *Does not* run evaluation.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n################################################################################\n#                             main                                             #\n################################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -> None:\n    # Ensure the output directory exists -------------------------------------\n    out_root = Path(hydra.utils.to_absolute_path(cfg.results_dir))\n    out_root.mkdir(parents=True, exist_ok=True)\n\n    overrides: List[str] = [\n        f\"run={cfg.run._config_name_}\",  # select the **same** run variant\n        f\"results_dir={cfg.results_dir}\",\n    ]\n    if cfg.trial_mode:\n        overrides.append(\"trial_mode=true\")\n        overrides.append(\"wandb.mode=disabled\")\n    else:\n        overrides.append(f\"wandb.mode={cfg.wandb.mode}\")\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\", *overrides]\n    print(\"[main] Launching:\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"boil-experiment\"\nversion = \"0.1.0\"\nrequires-python = \">=3.9\"\ndependencies = [\n  \"torch>=2.0\",\n  \"torchvision>=0.15\",\n  \"hydra-core>=1.3\",\n  \"wandb>=0.15\",\n  \"botorch>=0.8.5\",\n  \"gpytorch>=1.11\",\n  \"optuna>=3.4\",\n  \"numpy>=1.24\",\n  \"pandas>=2.0\",\n  \"matplotlib>=3.7\",\n  \"seaborn>=0.12\",\n  \"scikit-learn>=1.3\",\n  \"scipy>=1.10\"\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "config_yaml": "# config/config.yaml – global Hydra configuration\n# All *experiment-agnostic* defaults live in here.  The *actual* experimental\n# parameters are provided via the ``run`` config-group (see config/run/*.yaml).\n\ndefaults:\n  - _self_\n  - run: ??          # must be specified on the command-line (run id)\n\n###############################################################################\n#                               WandB                                          #\n###############################################################################\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online            # automatically set to \"disabled\" in trial_mode\n\n###############################################################################\n#                          training + data defaults                            #\n###############################################################################\n#   These act purely as *fallbacks* – they will be overwritten by each run's\n#   dedicated YAML (config/run/<run-id>.yaml).\n###############################################################################\nmodel:\n  name: two-layer-mlp-1.2m\n  input_dim: 784\n  hidden_units: 256\n  output_classes: 10\n  activation: relu\n  weight_init: kaiming_uniform\n\ndataset:\n  name: fashion-mnist\n  transforms:\n    - ToTensor\n    - Normalize(mean=0.5, std=0.5)\n  validation_split: 0.1\n\ntraining:\n  batch_size: 128\n  optimizer: adam\n  device: cuda\n  learning_rate: 1e-3\n  epochs_per_iteration: 5\n  num_workers: 4\n  pin_memory: true\n\n###############################################################################\n#                     BOIL / S-BOIL default meta-params                       #\n###############################################################################\nexperiment:\n  n_init_points: 3\n  total_evaluations: 30\n  acquisition: boil\n  beta_default: 1.0\n\noptuna:\n  n_trials: 30\n  search_space:\n    learning_rate: {type: loguniform, low: 1e-4, high: 1e-1}\n    hidden_units:   {type: int,        low: 64,  high: 512}\n    training_iteration_budget: {type: int, low: 1, high: 15}\n    beta: {type: categorical, choices: [0.5, 1.0]}\n\n###############################################################################\n#                     misc top-level knobs (CLI convenience)                  #\n###############################################################################\nresults_dir: outputs\ntrial_mode: false"
}
